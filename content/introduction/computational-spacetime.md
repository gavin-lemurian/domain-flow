+++
weight = 4
title = "Computational Spacetime"
date = "2017-02-15T06:58:22-05:00"
toc = true
next = "/introduction/freeschedule"
prev = "/introduction/domain-flow"
WebGLRenderTarget = true
IndexSpaceVisualization = true
RenderTargetName = "index_space_view"

tags = [ "domain flow algorithm", "matrix-multiply", "index-space", "lattice", "computational spacetime" ]
categories = [ "domain flow", "introduction" ]
series = [ "introduction" ]

+++

If you try to visualize the 'world' from the perspective of an operand flowing through a machine, you realize
that a physical machine creates a specific spatial constraint for the movement of program and data. 
Processing nodes are fixed in space, and information is exchanged between nodes to accomplish some transformation;
we have nodes to generate operands and communication lanes between the nodes to send operands (or programs) around.
This leads to a simple abstraction of a parallel machine consisting of a fixed graph in space that constrains
computation and communication events generated by the parallel algorithm. 

Just like it takes time to compute, it takes time to communicate. 
In an expression, such as, $c = a + b$ we are familiar with the progression of time for the add operation, 
but the assignment operation, which represents the exchange of information/communication phase,
is taking time just the same. It is this time relationship, the ratio between computational delay and communication delay
that impacts the structure of the optimal parallel algorithm.

Technology impacts the performance of both computation and communication, but in very different ways. Typically,
computational performance, being governed by miniaturization of VLSI manufacturing process geometries, improves
more rapidly than communication performance, which tends to be governed by system integration constraints that
are external to the chip. If we revisit the spatial graph of a parallel machine in our thought experiment, an
improvement in the performance of the computation relative to the communication of information changes the trade-off
between computation and communication in the parallel algorithm. In the vocabulary of spacetime, remote nodes
move farther away. Spacetime has another geometric interpretation of the relationship between space and time, called
the spacetime light cone, as shown here:

{{< figure src="/images/spacetime-light-cone.png" title="Spacetime Light Cone" >}}

Program and data movement in a parallel machine also experience this separation in spacetime. The propagation delays
in the communication channels act in the same way as the constraint on the speed of light in spacetime. Let's bring back
the mental model of a parallel machine being a fixed graph in space with communication channels connecting a subset
of nodes. The channels of communication constrain the propagation directions for information exchange. If we make
the assumption that computation and communication are separate steps and that they can be overlapped to hide the latency
of the communication phase, then we can leverage the same mental model of spacetime to argue about partial orders
of computational events that might be able to effect each other. We call this the _computational spacetime_ of the
machine. An operand cannot be delivered to another node unless it falls inside the future cone of the computational
spacetime defined by the distance an operand can travel along the communication channels in the time it took to
compute the operand. 

As designers, we have the option to try to compute more, and thus take more time, as to broaden the computational 
spacetime cone and be able to exchange information to nodes that are farther away. 
This is the act of blocking algorithms if you are an algorithm designer,
and it is the act of building big processors with lots of storage if you are a computer designer. But we also
have the option to compute less, and organize the operands in space so that they fall in the future cone of
the computational event. That is the act of designing domain flow algorithms if you are an algorithm designer, and the
act of building fine-grain computational networks if you are a computer designer. 




None of these approaches are a panacea for parallel computation Sometimes, a few big nodes with big communication pipes
is the best organization, and sometimes, a network of tightly-coupled multiply-add functional units is the best
organization.



create an abstract programming model that will be invariant to the underlying execution model.



Remember, nodes in the graph represent computational events, and links in the graph represent information exchanges.
Imagine that we have as many computational resources as there are nodes in the single assignment form
computational graph. In that case, we can simple 'embed' the computational graph in 3D space.
However, since a physical machine that would be able to evaluate a node in the graph will have
some physical extent, a collection of physical nodes will fill 3D space in some 'regular', crystalline
pattern. Or if we are conceptualizing the space presented by a VLSI chip's surface, it will be a 2D space.
These crystalline patterns are typically referred to as a _lattice_, and thus
the design of a domain flow algorithm is the act of finding space, time, and energy efficient embeddings
of some computational graph in N-dimensional space.

Back to our matrix multiply; we can now reinterpret the domain flow algorithm as a physical embedding.
Each index range, that is, the $i$, $j$, and $k$ in the constraint set, can be seen as a dimension in 3D space.
The index tag, such as $[i,j,k]$, is a location in 3D space, more accurately,
a location in the 3D Cartesian lattice, $\mathbb{N}^3$.

This is what the lattice for matmul looks like for a given N:

<canvas id="c"></canvas>

<div id="index_space_view"></div>

